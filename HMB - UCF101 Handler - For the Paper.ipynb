{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF101 Handler - For the Paper\n",
    "\n",
    "## Source:\n",
    "https://www.crcv.ucf.edu/data/UCF101.php<br>\n",
    "\n",
    "## RGB Images:\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.001<br>\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.002<br>\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_jpegs_256.zip.003<br>\n",
    "\n",
    "## Optical Flow:\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.001<br>\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.002<br>\n",
    "wget http://ftp.tugraz.at/pub/feichtenhofer/tsfusion/data/ucf101_tvl1_flow.zip.003<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, random, os, math, shutil, time, warnings, glob, \\\n",
    "  threading, pickle\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from scipy import ndimage\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import Process\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, \\\n",
    "  EarlyStopping, CSVLogger, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "from tensorflow.keras.layers import Input, Average, LSTM, \\\n",
    "  Dense, Dropout, Activation, Flatten, SpatialDropout2D, Reshape, \\\n",
    "  GlobalAveragePooling2D, Conv2D, TimeDistributed, \\\n",
    "  MaxPooling2D, BatchNormalization, GlobalAveragePooling1D\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling3D, ConvLSTM2D\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras.callbacks\n",
    "import tensorflow.keras.utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "def warn(*args, **kwargs): pass\n",
    "warnings.warn = warn\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class threadsafe_iterator:\n",
    "  def __init__(self, iterator):\n",
    "    self.iterator = iterator\n",
    "    self.lock = threading.Lock()\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    with self.lock:\n",
    "      return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "  \"\"\"Decorator\"\"\"\n",
    "  def gen(*a, **kw):\n",
    "    return threadsafe_iterator(func(*a, **kw))\n",
    "  return gen\n",
    "    \n",
    "def recall_m(y_true, y_pred):\n",
    "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "  recall = true_positives / (possible_positives + K.epsilon())\n",
    "  return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "  precision = true_positives / (predicted_positives + K.epsilon())\n",
    "  return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "  precision = precision_m(y_true, y_pred)\n",
    "  recall = recall_m(y_true, y_pred)\n",
    "  return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 10:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "\n",
    "def ComputeTVL1(prev, current, bound=15):\n",
    "  \"\"\"Compute the TV-L1 optical flow.\"\"\"\n",
    "  TVL1 = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "  flow = TVL1.calc(prev, current, None)\n",
    "  assert flow.dtype == np.float32\n",
    "  flow = (flow + bound) * (255.0 / (2*bound))\n",
    "  flow = np.round(flow).astype(int)\n",
    "  flow[flow >= 255] = 255\n",
    "  flow[flow <= 0] = 0\n",
    "  return flow\n",
    "\n",
    "def AugmentFrame(frame, inputShape, rotate=10, shearRange=5,\n",
    "    zoom=5, horizontalFlip=False, translationRange=5, \n",
    "    changeBrightness=True, cval=255):\n",
    "  assert frame is not None\n",
    "  rows, cols, ch = inputShape   \n",
    "  if (random.random() >= 0.5 and rotate): # Rotate\n",
    "    angle = random.randint(-rotate, rotate)\n",
    "    frame = frame.astype(np.uint8)\n",
    "    frame = ndimage.rotate(frame, angle, cval=255)\n",
    "  if (random.random() >= 0.5 and translationRange): # Translation\n",
    "    trX = translationRange*np.random.uniform()-translationRange/2\n",
    "    trY = translationRange*np.random.uniform()-translationRange/2\n",
    "    transM = np.float32([[1, 0, trX],[0, 1, trY]])\n",
    "    frame = cv2.warpAffine(frame, transM, (cols, rows), \n",
    "                           borderValue=(cval, cval, cval))\n",
    "  if (random.random() >= 0.5 and shearRange): # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "    pt1 = 5+shearRange*np.random.uniform()-shearRange/2\n",
    "    pt2 = 20+shearRange*np.random.uniform()-shearRange/2\n",
    "    pts2 = np.float32([[pt1, 5], [pt2, pt1], [5, pt2]])\n",
    "    shearM = cv2.getAffineTransform(pts1, pts2)\n",
    "    frame = cv2.warpAffine(frame, shearM, (cols, rows), \n",
    "                           borderValue=(cval, cval, cval))\n",
    "  if (ch == 3 and random.random() >= 0.55 and changeBrightness): # Brightness\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)\n",
    "    randomBright = 0.25 + np.random.uniform()\n",
    "    frame[:,:,2] = frame[:,:,2] * randomBright\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_HSV2RGB)\n",
    "  if (random.random() >= 0.60 and horizontalFlip): # H-Flip\n",
    "    frame = cv2.flip(frame, 1)\n",
    "  if (random.random() >= 0.65 and zoom): # Zoom:\n",
    "    width, height = inputShape[:2]\n",
    "    centerX, centerY = int(height/2), int(width/2)\n",
    "    radiusX, radiusY = int(zoom*height/100), int(zoom*width/100)\n",
    "    minX, maxX = centerX-radiusX, centerX+radiusX\n",
    "    minY, maxY = centerY-radiusY, centerY+radiusY\n",
    "    cropped = frame[minX:maxX, minY:maxY]\n",
    "    frame = cv2.resize(frame, inputShape[:2],\n",
    "      interpolation = cv2.INTER_AREA)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCF101Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideosDataset(tensorflow.keras.utils.Sequence):\n",
    "  def __init__(self, datasetPath, datasetKeyword, splits=(0.8, 0.1, 0.1),\n",
    "               batchSize=64, inputShape=(100, 100, 3), isGray=False, \n",
    "               storeCSV=True, storageDir=\"\", debug=True, extension=\"avi\", \n",
    "               shuffle=True, maxInMemory=5000, segmentation=1, packets=1):\n",
    "    self.datasetPath = datasetPath         # Videos Path\n",
    "    self.splits = splits     # Train - Test - Validation\n",
    "    self.storeCSV = storeCSV # Store CSV files?\n",
    "    self.datasetKeyword = datasetKeyword\n",
    "    self.storageDir = storageDir\n",
    "    self.debug = debug\n",
    "    self.extension = extension\n",
    "    self.maxInMemory = maxInMemory\n",
    "    self.shuffle = shuffle\n",
    "    self.inputShape = inputShape\n",
    "    self.batchSize = batchSize\n",
    "    self.isGray = isGray\n",
    "    self.segmentation = segmentation\n",
    "    self.packets = packets\n",
    "    self.metadata = None\n",
    "    \n",
    "    if (self.storeCSV):\n",
    "      self.metadata = os.path.join(storageDir, \n",
    "        self.datasetKeyword + \"_All_Metadata.csv\")\n",
    "      self.trainMetadata = os.path.join(storageDir, \n",
    "        self.datasetKeyword + \"_Train_Metadata.csv\")\n",
    "      self.testMetadata = os.path.join(storageDir, \n",
    "        self.datasetKeyword + \"_Test_Metadata.csv\")\n",
    "      self.validationMetadata = os.path.join(storageDir, \n",
    "        self.datasetKeyword + \"_Validation_Metadata.csv\")\n",
    "\n",
    "    self.framesDir = os.path.join(storageDir, \n",
    "      self.datasetKeyword + \"_Frames\")\n",
    "\n",
    "    self.flowsDir = os.path.join(storageDir, \n",
    "      self.datasetKeyword + \"_Flows\")\n",
    "\n",
    "    self.trainRecords = []\n",
    "    self.testRecords = []\n",
    "    self.validationRecords = []\n",
    "  \n",
    "    self.records = []\n",
    "    self.classes = []\n",
    "    self.noOfClasses = 0\n",
    "    \n",
    "    self.videosInfo = []\n",
    "    self.classesVideos = {}\n",
    "\n",
    "  def GetNoOfClasses(self):\n",
    "    return self.noOfClasses\n",
    "\n",
    "  def ProcessVideosMetadata(self):\n",
    "    if(self.debug): print(\"[STARTED] ProcessVideosMetadata.\")\n",
    "    \n",
    "    videoNames = glob.glob(self.datasetPath + \"/*.\" + self.extension)\n",
    "    if(self.debug): print(\"[INFO] There are %d videos.\" % len(videoNames))\n",
    "    \n",
    "    if (len(videoNames) <= 0): \n",
    "      if(self.debug): \n",
    "        print(\"[WARNING] There are no files in this path.\")\n",
    "        print(\"[COMPLETED] processVideosMetadata.\")\n",
    "      return\n",
    "\n",
    "    for i, videoPath in enumerate(videoNames):\n",
    "      videoPath = videoPath.strip()\n",
    "      extension = videoPath.split(\".\")[-1]\n",
    "      if (extension != self.extension): continue\n",
    "      videoName = os.path.basename(videoPath)\n",
    "      splitName = videoName.split(\"_\")\n",
    "      className = splitName[1].strip()\n",
    "      g = splitName[2].strip()\n",
    "      c = splitName[3].strip().split(\".\")[0]\n",
    "      gc = g + \"_\" + c\n",
    "\n",
    "      record = (i + 1, videoName, className, g, c, gc, extension)\n",
    "\n",
    "      if (className not in self.classes):\n",
    "        self.classes.append(className)\n",
    "        self.classesVideos[className] = []\n",
    "      self.classesVideos[className].append(record)\n",
    "      self.records.append(record)\n",
    "\n",
    "    self.classes = sorted(list(set(self.classes)))\n",
    "    self.noOfClasses = len(self.classes)\n",
    "\n",
    "    if (self.storeCSV):\n",
    "      if(self.debug): print(\"[INFO] There are %d records.\" % len(self.records))\n",
    "      df = pd.DataFrame(self.records, \n",
    "        columns=[\"number\", \"video_name\", \"class_name\",\n",
    "                \"g\", \"c\", \"g_c\", \"extension\"])\n",
    "      df.to_csv(self.metadata, index=False)\n",
    "    if(self.debug): print(\"[COMPLETED] ProcessVideosMetadata.\")\n",
    "\n",
    "  def SplitDataset(self):\n",
    "    if(self.debug): print(\"[STARTED] SplitDataset.\")\n",
    "    columns = [\"number\", \"video_name\", \"class_name\", \"g\", \"c\", \"g_c\", \"extension\"]\n",
    "    df = pd.read_csv(self.metadata)\n",
    "    self.records = list(df.values)\n",
    "    self.classes = list(sorted(set(df[\"class_name\"])))\n",
    "    if(self.debug): print(\"[INFO] There are\", len(self.classes), \"classes.\")\n",
    "    for cls in self.classes:\n",
    "        self.classesVideos[cls] = []\n",
    "    for record in self.records:\n",
    "      self.classesVideos[record[2]].append(record)\n",
    "    \n",
    "    trainRatio = self.splits[0]\n",
    "    testRatio = self.splits[1]\n",
    "    validationRatio = self.splits[2]\n",
    "    \n",
    "    for className in self.classes:\n",
    "      shuffledRecords = self.classesVideos[className][:]\n",
    "      random.shuffle(shuffledRecords)\n",
    "      classDict = {}\n",
    "      for record in shuffledRecords:\n",
    "        if (record[3] not in classDict.keys()):\n",
    "            classDict[record[3]] = []\n",
    "        classDict[record[3]].append(record)\n",
    "      for k in classDict.keys():\n",
    "        L = len(classDict[k])\n",
    "        trainIndex = math.ceil(L * trainRatio)\n",
    "        testIndex = math.floor(L * testRatio)\n",
    "        self.trainRecords.extend(classDict[k][:trainIndex])\n",
    "        self.testRecords.extend(classDict[k][trainIndex:trainIndex+testIndex])\n",
    "        self.validationRecords.extend(classDict[k][trainIndex+testIndex:])\n",
    "\n",
    "    if(self.debug): \n",
    "      print(\"[INFO] There are\", len(self.trainRecords), \"videos for training.\")\n",
    "      print(\"[INFO] There are\", len(self.validationRecords), \"videos for validation.\")\n",
    "      print(\"[INFO] There are\", len(self.testRecords), \"videos for testing.\")\n",
    "\n",
    "    if (self.storeCSV):\n",
    "      df = pd.DataFrame(self.trainRecords, columns=columns)\n",
    "      df.to_csv(self.trainMetadata, index=False)\n",
    "      df = pd.DataFrame(self.testRecords, columns=columns)\n",
    "      df.to_csv(self.testMetadata, index=False)\n",
    "      df = pd.DataFrame(self.validationRecords, columns=columns)\n",
    "      df.to_csv(self.validationMetadata, index=False)\n",
    "    if(self.debug): print(\"[COMPLETED] SplitDataset.\")\n",
    "    \n",
    "  def ReadVideosMetadata(self):\n",
    "    if(self.debug): print(\"[STARTED] ReadVideosMetadata.\")\n",
    "    df = pd.read_csv(self.metadata)\n",
    "    self.records = list(df.values)\n",
    "    self.classes = list(sorted(set(df[\"class_name\"])))\n",
    "    for record in self.records:\n",
    "      if (record[2] not in self.classesVideos.keys()):\n",
    "        self.classesVideos[record[2]] = []\n",
    "      self.classesVideos[record[2]].append(record)\n",
    "    df = pd.read_csv(self.trainMetadata)\n",
    "    self.trainRecords = list(df.values)\n",
    "    df = pd.read_csv(self.testMetadata)\n",
    "    self.testRecords = list(df.values)\n",
    "    df = pd.read_csv(self.validationMetadata)\n",
    "    self.validationRecords = list(df.values)\n",
    "    self.noOfClasses = len(self.classes)\n",
    "    if(self.debug): \n",
    "      print(\"[INFO] There are\", len(self.trainRecords), \"videos for training.\")\n",
    "      print(\"[INFO] There are\", len(self.validationRecords), \"videos for validation.\")\n",
    "      print(\"[INFO] There are\", len(self.testRecords), \"videos for testing.\")\n",
    "      print(\"[COMPLETED] ReadVideosMetadata.\")\n",
    "    \n",
    "  def ManipulateVideosToFrames(self, which):\n",
    "    if(self.debug): print(\"[STARTED] ManipulateVideosToFrames.\")\n",
    "    \n",
    "    if (which == \"train\"):\n",
    "      records = self.trainRecords\n",
    "    elif(which == \"test\"):\n",
    "      records = self.testRecords\n",
    "    elif(which == \"validation\"):\n",
    "      records = self.validationRecords\n",
    "    else:\n",
    "      return\n",
    "    folder = self.framesDir\n",
    "    \n",
    "    if (os.path.exists(folder)): shutil.rmtree(folder)\n",
    "    if (not os.path.exists(folder)): os.mkdir(folder)\n",
    "    if(self.debug): print(\"[INFO] Starting to work on [%s].\" % which)\n",
    "\n",
    "    for i, record in enumerate(records):\n",
    "      columns = [\"number\", \"video_name\", \"class_name\", \"g\", \"c\", \"g_c\", \"extension\"]\n",
    "      frames = []\n",
    "      classPath = os.path.join(folder, record[2])\n",
    "      gcPath = os.path.join(classPath, record[5])\n",
    "      if (not os.path.exists(classPath)): os.mkdir(classPath)\n",
    "      if (not os.path.exists(gcPath)): os.mkdir(gcPath)      \n",
    "      videoPath = os.path.join(self.path, record[1])\n",
    "      video = cv2.VideoCapture(videoPath)\n",
    "      success, img = video.read()\n",
    "      count = 0\n",
    "      while success:\n",
    "        storePath = os.path.join(gcPath, \n",
    "          (\"frame_%s_%d_%d.jpg\" % (record[5], i + 1, count + 1)))\n",
    "        cv2.imwrite(storePath, img)   \n",
    "        success, img = video.read()\n",
    "        count += 1\n",
    "      print(\"[INFO] [%d/%d] %s has completed processing with [%d] frames.\" % \\\n",
    "            (i+1, len(records), record[1], count))\n",
    "    if(self.debug): print(\"[COMPLETED] ManipulateVideosToFrames.\")\n",
    "\n",
    "        \n",
    "  @threadsafe_generator\n",
    "  def LiveGeneratorImproved(self, which, subWhich, noOfThreads=5,\n",
    "      doAugmentation=True, rotate=10, shearRange=5,\n",
    "      zoom=5, horizontalFlip=True, translationRange=5, \n",
    "      changeBrightness=True):\n",
    "    if (self.debug): print(\"[STARTED] LiveGeneratorImproved\")\n",
    "\n",
    "    if (which == \"train\"): \n",
    "        records = self.trainRecords\n",
    "    elif(which == \"test\"): \n",
    "        records = self.testRecords\n",
    "    elif(which == \"validation\"): \n",
    "        records = self.validationRecords\n",
    "    elif(which == \"all\"): \n",
    "        records = self.records\n",
    "    else: return\n",
    "    frames = self.framesDir\n",
    "    flows = self.flowsDir\n",
    "    \n",
    "    whichElement = which\n",
    "    subWhichElement = subWhich\n",
    "    \n",
    "    random.shuffle(records)\n",
    "    recordsTemp = records[:]\n",
    "    if (self.debug): print(\"Working on:\", which)\n",
    "    \n",
    "    class Data(object):\n",
    "        def __init__(self, records, recordsTemp):\n",
    "            self.XY = []\n",
    "            self.records = records\n",
    "            self.recordsTemp = recordsTemp\n",
    "    \n",
    "    XYobj = Data(records, recordsTemp)\n",
    "\n",
    "    class myThread (threading.Thread):\n",
    "      def __init__(self, XYobj, frames, flows, whichElement, \n",
    "                  subWhichElement, classes, isGray,\n",
    "                  inputShape, doAugmentation,\n",
    "                  rotate, shearRange, zoom, horizontalFlip, \n",
    "                  translationRange, changeBrightness, shuffle,\n",
    "                  maxInMemory, segmentation, packets):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.XYobj = XYobj\n",
    "        self.classes = classes\n",
    "        self.doAugmentation = doAugmentation\n",
    "        self.rotate = rotate\n",
    "        self.zoom = zoom\n",
    "        self.shearRange = shearRange\n",
    "        self.horizontalFlip = horizontalFlip\n",
    "        self.changeBrightness = changeBrightness\n",
    "        self.translationRange = translationRange\n",
    "        self.inputShape = inputShape\n",
    "        self.frames = frames\n",
    "        self.flows = flows\n",
    "        self.whichElement = whichElement\n",
    "        self.subWhichElement = subWhichElement\n",
    "        self.isGray = isGray\n",
    "        self.shuffle = shuffle\n",
    "        self.maxInMemory = maxInMemory\n",
    "        self.segmentation = segmentation\n",
    "        self.packets = packets\n",
    "        \n",
    "      def run(self):\n",
    "        columns = [\"number\", \"video_name\", \"class_name\", \"g\", \"c\", \"g_c\", \"extension\"]\n",
    "        while(True):\n",
    "          while (len(XYobj.XY) > self.maxInMemory):\n",
    "            time.sleep(random.random())\n",
    "            continue\n",
    "          if (len(XYobj.recordsTemp) <= 0):\n",
    "            print(\"[INFO] Filling the records again for %s.\" % self.whichElement)\n",
    "            XYobj.recordsTemp = XYobj.records[:]\n",
    "            if (self.shuffle): random.shuffle(XYobj.recordsTemp)\n",
    "          try:\n",
    "            \n",
    "            if (self.subWhichElement[0] == \"flows\"):\n",
    "              record = XYobj.recordsTemp.pop(0)  \n",
    "              videoName = record[1]\n",
    "              className = record[2]\n",
    "              videoNoExtName = record[1].split(\".\")[0]\n",
    "              uFolderPath = os.path.join(flows, \"u\", videoNoExtName) \n",
    "              vFolderPath = os.path.join(flows, \"v\", videoNoExtName) \n",
    "              classIndex = self.classes.index(className)\n",
    "              oneHotClass = to_categorical(classIndex, len(self.classes))\n",
    "              assert oneHotClass.shape[0] == len(self.classes)\n",
    "            \n",
    "              uDirFrames = os.listdir(uFolderPath)\n",
    "              Lu = len(uDirFrames)\n",
    "              vDirFrames = os.listdir(vFolderPath)\n",
    "              Lv = len(vDirFrames)\n",
    "            \n",
    "              if (self.subWhichElement[1] == \"single\"):\n",
    "                for j in range(self.packets):\n",
    "                  start = random.randint(0, Lu - int(self.segmentation / 2))\n",
    "                  if (start <= 0): start = 0\n",
    "                  stack = []\n",
    "                  for i in range(int(self.segmentation / 2)):\n",
    "                    uImagePath = os.path.join(uFolderPath, uDirFrames[i+start])\n",
    "                    vImagePath = os.path.join(vFolderPath, vDirFrames[i+start])\n",
    "                    if (self.isGray): \n",
    "                      uFrame = cv2.imread(uImagePath, cv2.IMREAD_GRAYSCALE)\n",
    "                      vFrame = cv2.imread(vImagePath, cv2.IMREAD_GRAYSCALE)\n",
    "                    else: \n",
    "                      uFrame = cv2.imread(uImagePath)\n",
    "                      vFrame = cv2.imread(vImagePath)\n",
    "                    uFrame = cv2.resize(uFrame, self.inputShape[:2], interpolation=cv2.INTER_AREA)\n",
    "                    vFrame = cv2.resize(vFrame, self.inputShape[:2], interpolation=cv2.INTER_AREA)\n",
    "                    stack.append(uFrame) \n",
    "                    stack.append(vFrame)\n",
    "                  stack = np.array(stack)\n",
    "                  stack = stack.astype('float32') / 255.0\n",
    "                  stack = np.swapaxes(stack, 0, 1)\n",
    "                  stack = np.swapaxes(stack, 1, 2)\n",
    "                  XYobj.XY.append((stack, oneHotClass))\n",
    "              elif (self.subWhichElement[1] == \"stacked\"):\n",
    "                overallStack = []\n",
    "                for j in range(self.packets):\n",
    "                  start = random.randint(0, Lu - int(self.segmentation / 2))\n",
    "                  if (start <= 0): start = 0\n",
    "                  stack = []\n",
    "                  for i in range(int(self.segmentation / 2)):\n",
    "                    uImagePath = os.path.join(uFolderPath, uDirFrames[i+start])\n",
    "                    vImagePath = os.path.join(vFolderPath, vDirFrames[i+start])\n",
    "                    if (self.isGray): \n",
    "                      uFrame = cv2.imread(uImagePath, cv2.IMREAD_GRAYSCALE)\n",
    "                      vFrame = cv2.imread(vImagePath, cv2.IMREAD_GRAYSCALE)\n",
    "                    else: \n",
    "                      uFrame = cv2.imread(uImagePath)\n",
    "                      vFrame = cv2.imread(vImagePath)\n",
    "                    uFrame = cv2.resize(uFrame, self.inputShape[:2], interpolation=cv2.INTER_AREA)\n",
    "                    vFrame = cv2.resize(vFrame, self.inputShape[:2], interpolation=cv2.INTER_AREA)\n",
    "                    stack.append(uFrame) \n",
    "                    stack.append(vFrame)\n",
    "                  stack = np.array(stack)\n",
    "                  stack = stack.astype('float32') / 255.0\n",
    "                  stack = np.swapaxes(stack, 0, 1)\n",
    "                  stack = np.swapaxes(stack, 1, 2)\n",
    "                  overallStack.append(stack)\n",
    "                XYobj.XY.append((overallStack, oneHotClass))\n",
    "            \n",
    "            elif (self.subWhichElement[0] == \"frames\"):\n",
    "              record = XYobj.recordsTemp.pop(0)  \n",
    "              className = record[2]\n",
    "              gcName = record[5]\n",
    "              folderPath = os.path.join(frames, className, gcName)    \n",
    "              classIndex = self.classes.index(className)\n",
    "              oneHotClass = to_categorical(classIndex, len(self.classes))\n",
    "              assert oneHotClass.shape[0] == len(self.classes)\n",
    "\n",
    "              dirFrames = os.listdir(folderPath)\n",
    "              L = len(dirFrames)\n",
    "              if (self.subWhichElement[1] == \"single\"):\n",
    "                if (self.packets > int(L / self.segmentation)):\n",
    "                    self.packets = int(L / self.segmentation)\n",
    "                for offset in range(self.packets):\n",
    "                  for i in range(self.segmentation):\n",
    "                    imagePath = os.path.join(folderPath, dirFrames[i + self.segmentation * offset])\n",
    "                    if (self.isGray): frame = cv2.imread(imagePath, cv2.IMREAD_GRAYSCALE)\n",
    "                    else: frame = cv2.imread(imagePath)\n",
    "                    frame = cv2.resize(frame, self.inputShape[:2], interpolation=cv2.INTER_AREA)\n",
    "                    XYobj.XY.append((frame, oneHotClass))\n",
    "\n",
    "                    if (self.doAugmentation):\n",
    "                      augFrame = AugmentFrame(frame, self.inputShape,\n",
    "                        self.rotate, self.shearRange, \n",
    "                        self.zoom, self.horizontalFlip, \n",
    "                        self.translationRange, self.changeBrightness)\n",
    "                      augFrame = cv2.resize(augFrame, self.inputShape[:2],\n",
    "                          interpolation=cv2.INTER_AREA)\n",
    "                      XYobj.XY.append((augFrame, oneHotClass))\n",
    "              elif (self.subWhichElement[1] == \"stacked\"):\n",
    "                if (self.packets > L - self.segmentation):\n",
    "                    self.packets = L - self.segmentation\n",
    "                for offset in range(self.packets):\n",
    "                  stack = []\n",
    "                  for i in range(self.segmentation):\n",
    "                    imagePath = os.path.join(folderPath, dirFrames[i + offset])\n",
    "                    if (self.isGray): frame = cv2.imread(imagePath, cv2.IMREAD_GRAYSCALE)\n",
    "                    else: frame = cv2.imread(imagePath)\n",
    "                    frame = cv2.resize(frame, self.inputShape[:2], interpolation=cv2.INTER_AREA)\n",
    "                    stack.append(frame)  \n",
    "                  stack = np.array(stack)\n",
    "                  stack = stack.astype('float32') / 255.0\n",
    "                  XYobj.XY.append((stack, oneHotClass))\n",
    "          except Exception as e:\n",
    "            print(\"\\nError:\", e)\n",
    "            pass\n",
    "\n",
    "    threads = []\n",
    "    if (len(threads) <= 0):\n",
    "      for i in range(noOfThreads):\n",
    "        threads.append(myThread(XYobj, frames, flows, \n",
    "          whichElement, subWhichElement, self.classes,\n",
    "          self.isGray, self.inputShape,\n",
    "          doAugmentation, rotate, shearRange, zoom, horizontalFlip, \n",
    "          translationRange, changeBrightness, \n",
    "          self.shuffle, self.maxInMemory, self.segmentation, self.packets))\n",
    "      for i in range(noOfThreads):\n",
    "        print(\"Starting \" + str(i+1) + \" Thread!\")\n",
    "        threads[i].start()\n",
    "\n",
    "    while(1):\n",
    "      if (len(XYobj.XY) >= self.batchSize):\n",
    "        XYt = XYobj.XY[:self.batchSize]\n",
    "        XYobj.XY = XYobj.XY[self.batchSize:]\n",
    "        if(self.shuffle):\n",
    "          random.shuffle(XYobj.XY)\n",
    "          random.shuffle(XYt)\n",
    "        Xt = [el[0] for el in XYt]\n",
    "        Xt = np.array(Xt)\n",
    "        if (subWhichElement[0] == \"frames\" and subWhichElement[1] == \"single\"):\n",
    "          Xt = np.array(Xt).reshape(-1, self.inputShape[0], \n",
    "            self.inputShape[1], self.inputShape[2])\n",
    "          Xt = Xt.astype('float32') / 255.0\n",
    "          \n",
    "        yt = [el[1] for el in XYt]\n",
    "        yt = np.array(yt)\n",
    "        yt = np.squeeze(yt)\n",
    "\n",
    "        yield Xt, yt\n",
    "    if (self.debug): print(\"[COMPLETED] LiveGeneratorImproved\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models(object):\n",
    "  def __init__(self, inputShape, noOfClasses,\n",
    "               hiddenActivation, outputActivation,\n",
    "               optimizer, weightsInit, batchSize, noOfEpochs,\n",
    "               loss, metrics, stepsPerEpoch, validationSteps,\n",
    "               loadModel=False,\n",
    "               datasetKeyword=None,\n",
    "               resultsFolder=None, modelName=None,\n",
    "               savedFolder=None, savedHDF5=None, \n",
    "               weights='imagenet', segmentation=1, packets=1):\n",
    "    self.inputShape = inputShape\n",
    "    self.hiddenActivation = hiddenActivation\n",
    "    self.outputActivation = outputActivation\n",
    "    self.weightsInit = weightsInit\n",
    "    self.noOfClasses = noOfClasses\n",
    "    self.loadModel = loadModel\n",
    "    self.resultsFolder = resultsFolder\n",
    "    self.savedFolder = savedFolder\n",
    "    self.savedHDF5 = savedHDF5\n",
    "    self.modelName = modelName\n",
    "    self.optimizer = optimizer\n",
    "    self.batchSize = batchSize\n",
    "    self.noOfEpochs = noOfEpochs\n",
    "    self.stepsPerEpoch = stepsPerEpoch\n",
    "    self.validationSteps = validationSteps\n",
    "    self.loss = loss\n",
    "    self.metrics = metrics\n",
    "    self.model = None\n",
    "    self.datasetKeyword = datasetKeyword\n",
    "    self.weights = weights\n",
    "    self.segmentation = segmentation\n",
    "    self.packets = packets\n",
    "\n",
    "    assert self.modelName is not None\n",
    "    assert self.datasetKeyword is not None\n",
    "    assert self.resultsFolder is not None\n",
    "    assert self.noOfClasses > 0\n",
    "\n",
    "    if (self.loadModel): \n",
    "        self.model = self.LoadModel()\n",
    "        self.time = self.savedFolder.split(\"-\")[-1]\n",
    "    else:\n",
    "      if (self.modelName == \"SpatialCNNnLSTM\"): self.model = self.SpatialCNNnLSTM()\n",
    "      elif (self.modelName == \"TemporalCNNnLSTM\"): self.model = self.TemporalCNNnLSTM()\n",
    "      elif (self.modelName == \"TemporalCNNModel\"): self.model = self.TemporalCNNModel()\n",
    "      elif (self.modelName == \"DenseNet201Model\"): self.model = self.DenseNet201Model()\n",
    "      elif (self.modelName == \"SpatialSmallCNNnLSTM\"): self.model = self.SpatialSmallCNNnLSTM()\n",
    "      \n",
    "      self.time = time.strftime(\"%y%m%d%H%M\", time.localtime())\n",
    "      self.savedFolder = os.path.join(self.resultsFolder, \n",
    "        self.datasetKeyword + \"-\" + self.modelName + \"-\" + self.time)\n",
    "      if not os.path.exists(self.savedFolder):\n",
    "        print(\"Creating:\", self.savedFolder)\n",
    "        os.makedirs(self.savedFolder)\n",
    "      else: print(\"Created Previously:\", self.savedFolder)\n",
    "      \n",
    "    self.PrepareCallbacks()\n",
    "\n",
    "  def PrepareCallbacks(self):\n",
    "    # Callbacks: Save the model.\n",
    "    filepath = \"{}-{}-{}\".format(self.datasetKeyword, self.modelName, self.time)\n",
    "    filepath = filepath + \"-{epoch:03d}-{val_accuracy:.4f}-{val_loss:.3f}.hdf5\"\n",
    "\n",
    "    class TrainHistory(tensorflow.keras.callbacks.Callback):\n",
    "      def __init__(self, obj, logs={}):\n",
    "        self.logs = []\n",
    "        self.obj = obj\n",
    "      def on_train_begin(self, logs=None):\n",
    "        self.startTime = time.time()\n",
    "      def on_train_end(self, logs=None):\n",
    "        self.endTime = time.time()\n",
    "        s = str(self.startTime) + \"\\n\"\n",
    "        s += str(self.endTime) + \"\\n\"\n",
    "        deltaTime = self.endTime - self.startTime\n",
    "        hours, rem = divmod(deltaTime, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        s += \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds)\n",
    "        f = open(os.path.join(self.obj.savedFolder, \n",
    "                  \"time-\" + self.obj.time + \".data\"), \"a\") \n",
    "        f.write(\"%s\\n\" % s)\n",
    "        f.close()\n",
    "      def on_epoch_end(self, epoch, logs={}):\n",
    "        obj = self.obj\n",
    "        data = [obj.datasetKeyword, obj.modelName, \n",
    "                str(obj.time), str(time.time()),\n",
    "                obj.weightsInit, obj.optimizer, \n",
    "                str(epoch), str(obj.noOfEpochs),\n",
    "                str(obj.inputShape[0]), str(obj.inputShape[1]), \n",
    "                obj.hiddenActivation, obj.outputActivation, \n",
    "                str(obj.noOfClasses), \n",
    "                str(obj.batchSize), obj.loss,\n",
    "                str(obj.stepsPerEpoch), str(obj.validationSteps)]\n",
    "        for k in logs.keys():\n",
    "          data.append(str(logs[k]))\n",
    "        s = ','.join(data)\n",
    "        f = open(os.path.join(obj.savedFolder, \n",
    "                              \"history-\" + obj.time + \".csv\"), \"a\") \n",
    "        f.write(\"%s\\n\" % s)\n",
    "        f.close()\n",
    "\n",
    "    chkValAcc = ModelCheckpoint(\n",
    "      filepath=os.path.join(self.savedFolder, filepath),\n",
    "      verbose=1,\n",
    "      monitor='val_accuracy',\n",
    "      save_best_only=True\n",
    "    )\n",
    "\n",
    "    chkValLoss = ModelCheckpoint(\n",
    "      filepath=os.path.join(self.savedFolder, filepath),\n",
    "      verbose=1,\n",
    "      monitor='val_loss',\n",
    "      save_best_only=True\n",
    "    )\n",
    "    \n",
    "    tb = TensorBoard(log_dir=os.path.join(self.savedFolder))\n",
    "    \n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.25,\n",
    "      patience=5, verbose=1, min_lr=0.001)\n",
    "\n",
    "    # Learning rate schedule.\n",
    "    lrSchedule = LearningRateScheduler(scheduler, verbose=1)\n",
    "    \n",
    "    # Callbacks: Early stoper.\n",
    "    earlyStopper = EarlyStopping(monitor='accuracy', patience=100)\n",
    "\n",
    "    # Callbacks: Save results.\n",
    "    csvLogger = CSVLogger(os.path.join(self.savedFolder, 'log-' + \\\n",
    "        str(self.time) + '.csv'), append=True)\n",
    "\n",
    "    self.callbacks = [earlyStopper, csvLogger, tb, reduceLR, lrSchedule,\n",
    "                      chkValAcc, chkValLoss, TrainHistory(self)]\n",
    "\n",
    "  def LoadModel(self):\n",
    "    path = self.savedFolder\n",
    "    hdf5 = os.path.join(path, self.savedHDF5)\n",
    "    assert os.path.exists(path)\n",
    "\n",
    "    dependencies = {\n",
    "      'recall_m': recall_m,\n",
    "      'precision_m': precision_m,\n",
    "      'f1_m': f1_m,\n",
    "    }\n",
    "    return load_model(hdf5, custom_objects=dependencies)\n",
    "\n",
    "  def CompileModel(self):\n",
    "    self.model.compile(\n",
    "      optimizer=self.optimizer,\n",
    "      loss=self.loss,\n",
    "      metrics=self.metrics,\n",
    "    )\n",
    "\n",
    "  def RefineCompileModel(self):\n",
    "    L = int(len(self.model.layers))\n",
    "    factor = int(2*L/3)\n",
    "    for layer in self.model.layers[factor:]: layer.trainable = True\n",
    "    for layer in self.model.layers[:factor]: layer.trainable = False\n",
    "    self.CompileModel()\n",
    "\n",
    "  def FitModel(self, trainGenerator, validationGenerator):\n",
    "    self.model.summary()\n",
    "    self.model.fit_generator(\n",
    "      generator=trainGenerator,\n",
    "      steps_per_epoch=self.stepsPerEpoch,\n",
    "      validation_data=validationGenerator,\n",
    "      validation_steps=self.validationSteps,\n",
    "      epochs=self.noOfEpochs,\n",
    "      callbacks=self.callbacks,\n",
    "      max_queue_size=1024,\n",
    "      use_multiprocessing=False,\n",
    "      workers=1,\n",
    "      verbose=1,\n",
    "    )\n",
    "\n",
    "  def EvaluateModel(self, testGenerator):\n",
    "    results = self.model.evaluate_generator(\n",
    "      generator=testGenerator,\n",
    "      steps=self.stepsPerEpoch,\n",
    "      max_queue_size=1024,\n",
    "      use_multiprocessing=False,\n",
    "      workers=1,\n",
    "      verbose=1,\n",
    "    )\n",
    "    print(self.model.metrics_names)\n",
    "    print(results)\n",
    "\n",
    "  def DenseNet201Model(self):\n",
    "    baseModel = DenseNet201(weights=self.weights, include_top=False)\n",
    "    x = baseModel.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation=self.hiddenActivation)(x) # FC\n",
    "    predictions = Dense(self.noOfClasses, activation=self.outputActivation)(x)\n",
    "    model = Model(inputs=baseModel.input, outputs=predictions)\n",
    "    L = int(len(model.layers))\n",
    "    for layer in model.layers[:]: layer.trainable = False\n",
    "    for layer in model.layers[L-4:]: layer.trainable = True\n",
    "    return  model\n",
    "\n",
    "  def TemporalCNNModel(self):\n",
    "    p = self.inputShape\n",
    "    shape = [p[0], p[1], self.segmentation]\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(96, (7, 7), strides=2, padding='same', input_shape=shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(self.hiddenActivation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(256, (5, 5), strides=2, padding='same'))\n",
    "    model.add(Activation(self.hiddenActivation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(512, (3, 3), strides=1, activation=self.hiddenActivation, padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), strides=1, activation=self.hiddenActivation, padding='same'))\n",
    "    model.add(Conv2D(512, (3, 3), strides=1, activation=self.hiddenActivation, padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation=self.hiddenActivation))\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Dense(2048, activation=self.hiddenActivation))\n",
    "    model.add(Dropout(0.9))\n",
    "    model.add(Dense(self.noOfClasses, activation=self.outputActivation))\n",
    "    return  model\n",
    "\n",
    "  def SpatialSmallCNNnLSTM(self):\n",
    "    p = self.inputShape\n",
    "    shape = [self.segmentation, p[0], p[1], p[2]]\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001)),\n",
    "      input_shape=shape))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(Conv2D(32, (3,3), \n",
    "      kernel_initializer=self.weightsInit,\n",
    "      kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(256, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(Conv2D(256, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "    model.add(TimeDistributed(Conv2D(512, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(Conv2D(512, (3, 3), padding='same',\n",
    "      kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "    model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(CuDNNLSTM(256, return_sequences=False))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(self.noOfClasses, activation=self.outputActivation))\n",
    "    return model\n",
    "\n",
    "  def TemporalCNNnLSTM(self):\n",
    "    p = self.inputShape\n",
    "    shape = [self.packets, p[0], p[1], self.segmentation]\n",
    "    return self.CNNnLSTM(shape)\n",
    "\n",
    "  def SpatialCNNnLSTM(self):\n",
    "    p = self.inputShape\n",
    "    shape = [self.segmentation, p[0], p[1], p[2]]\n",
    "    return self.CNNnLSTM(shape)\n",
    "\n",
    "  def CNNnLSTM(self, shape):\n",
    "      model = Sequential()\n",
    "      model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001)),\n",
    "        input_shape=shape))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(Conv2D(32, (3,3), \n",
    "        kernel_initializer=self.weightsInit,\n",
    "        kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "      model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "      model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "      model.add(TimeDistributed(Conv2D(256, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(Conv2D(256, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "      model.add(TimeDistributed(Conv2D(512, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(Conv2D(512, (3, 3), padding='same',\n",
    "        kernel_initializer=self.weightsInit, kernel_regularizer=l2(l=0.001))))\n",
    "      model.add(TimeDistributed(BatchNormalization()))\n",
    "      model.add(TimeDistributed(Activation(self.hiddenActivation)))\n",
    "      model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "      model.add(TimeDistributed(Flatten()))\n",
    "      model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "      model.add(Dense(self.noOfClasses, activation=self.outputActivation))\n",
    "      return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMAGE_SHAPE_64 = (64, 64, 3)\n",
    "IMAGE_SHAPE_80 = (80, 80, 3)\n",
    "IMAGE_SHAPE_100 = (100, 100, 3)\n",
    "IMAGE_SHAPE_128 = (128, 128, 3)\n",
    "IMAGE_SHAPE_256 = (256, 256, 3)\n",
    "GRAY_SHAPE_64 = (64, 64, 1)\n",
    "GRAY_SHAPE_100 = (100, 100, 1)\n",
    "GRAY_SHAPE_128 = (128, 128, 1)\n",
    "GRAY_SHAPE_256 = (256, 256, 1)\n",
    "WEIGHTS_IMAGENET = \"imagenet\"\n",
    "OUTPUT_ACTIVATION_SOFTMAX = \"softmax\"\n",
    "HIDDEN_ACTIVATION_RELU = \"relu\"\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "RMS_OPTIMIZER = \"rmsprop\"\n",
    "ADAM_OPTIMIZER = \"adam\"\n",
    "NADAM_OPTIMIZER = \"nadam\"\n",
    "ADAGRAD_OPTIMIZER = \"adagrad\"\n",
    "ADAMAX_OPTIMIZER = \"adamax\"\n",
    "ADADELTA_OPTIMIZER = \"adadelta\"\n",
    "SGD_OPTIMIZER = 'sgd'\n",
    "METRICS = ['accuracy', 'top_k_categorical_accuracy', \n",
    "           recall_m, precision_m, f1_m]\n",
    "STEPS_PER_EPOCH_16384 = 16384\n",
    "STEPS_PER_EPOCH_4096 = 4096\n",
    "STEPS_PER_EPOCH_1024 = 1024\n",
    "STEPS_PER_EPOCH_768 = 768\n",
    "STEPS_PER_EPOCH_512 = 512\n",
    "STEPS_PER_EPOCH_256 = 256\n",
    "STEPS_PER_EPOCH_128 = 128\n",
    "STEPS_PER_EPOCH_64 = 64\n",
    "STEPS_PER_EPOCH_32 = 32\n",
    "VALIDATION_STEPS_1024 = 1024\n",
    "VALIDATION_STEPS_512 = 512\n",
    "VALIDATION_STEPS_256 = 256\n",
    "VALIDATION_STEPS_128 = 128\n",
    "VALIDATION_STEPS_64 = 64\n",
    "VALIDATION_STEPS_32 = 32\n",
    "NO_OF_EPOCHS_4 = 4\n",
    "NO_OF_EPOCHS_8 = 8\n",
    "NO_OF_EPOCHS_16 = 16\n",
    "NO_OF_EPOCHS_32 = 32\n",
    "NO_OF_EPOCHS_64 = 64\n",
    "NO_OF_EPOCHS_128 = 128\n",
    "NO_OF_EPOCHS_150 = 150\n",
    "NO_OF_EPOCHS_256 = 256\n",
    "NO_OF_EPOCHS_512 = 512\n",
    "BATCH_SIZE_2048 = 2048\n",
    "BATCH_SIZE_1024 = 1024\n",
    "BATCH_SIZE_512 = 512\n",
    "BATCH_SIZE_256 = 256\n",
    "BATCH_SIZE_128 = 128\n",
    "BATCH_SIZE_64 = 64\n",
    "BATCH_SIZE_32 = 32\n",
    "BATCH_SIZE_16 = 16\n",
    "BATCH_SIZE_8 = 8\n",
    "BATCH_SIZE_4 = 4\n",
    "BATCH_SIZE_2 = 2\n",
    "LIMIT_SEGMENTATION_1 = 1\n",
    "LIMIT_SEGMENTATION_2 = 2\n",
    "LIMIT_SEGMENTATION_4 = 4\n",
    "LIMIT_SEGMENTATION_8 = 8\n",
    "LIMIT_SEGMENTATION_10 = 10\n",
    "LIMIT_SEGMENTATION_16 = 16\n",
    "LIMIT_SEGMENTATION_20 = 20\n",
    "LIMIT_SEGMENTATION_25 = 25\n",
    "LIMIT_SEGMENTATION_32 = 32\n",
    "LIMIT_SEGMENTATION_64 = 64\n",
    "LIMIT_SEGMENTATION_128 = 128\n",
    "LIMIT_SEGMENTATION_256 = 256\n",
    "SPLITS_70_15_15 = (0.70, 0.15, 0.15)\n",
    "SPLITS_80_10_10 = (0.80, 0.10, 0.10)\n",
    "OPTIMIZERS = ['adamax', 'sgd', 'adam', 'nadam', 'adadelta', 'adagrad']\n",
    "WEIGHTS_INITS = ['he_normal', 'he_uniform', 'glorot_normal', 'glorot_uniform',\n",
    "    'lecun_normal', 'lecun_uniform'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "PROJECT_FOLDER = \"\"\n",
    "DATASET_FOLDER = \"UCF_Videos\"\n",
    "RESULTS_FOLDER = DATASET_FOLDER + \"_Results\"\n",
    "PROJECT_FOLDER = os.path.join(os.getcwd(), PROJECT_FOLDER)\n",
    "DATASET_FOLDER = os.path.join(PROJECT_FOLDER, DATASET_FOLDER)\n",
    "RESULTS_FOLDER = os.path.join(PROJECT_FOLDER, RESULTS_FOLDER)\n",
    "DATASET_KEYWORD = \"UCF\"\n",
    "OPTIMIZER = OPTIMIZERS[0]\n",
    "WEIGHTS_INIT = WEIGHTS_INITS[3]\n",
    "SPLITS = SPLITS_70_15_15\n",
    "HIDDEN_ACTIVATION = HIDDEN_ACTIVATION_RELU\n",
    "OUTPUT_ACTIVATION = OUTPUT_ACTIVATION_SOFTMAX\n",
    "IMAGE_SHAPE = IMAGE_SHAPE_100\n",
    "BATCH_SIZE = BATCH_SIZE_32\n",
    "NO_OF_EPOCHS = NO_OF_EPOCHS_16\n",
    "STEPS_PER_EPOCH = STEPS_PER_EPOCH_16384\n",
    "VALIDATION_STEPS = VALIDATION_STEPS_1024\n",
    "LOAD_MODEL = False\n",
    "SAVED_FOLDER = \"\"\n",
    "SAVED_FOLDER = os.path.join(RESULTS_FOLDER, SAVED_FOLDER)\n",
    "SAVED_HDF5 = \"\"\n",
    "DEBUG = True\n",
    "SHUFFLE = True\n",
    "AUGMENT = True\n",
    "MAX_IN_MEMORY = 10000\n",
    "IS_GRAY = False\n",
    "MODEL_NAME = \"SpatialSmallCNNnLSTM\" \n",
    "LIMIT_SEGMENTATION = LIMIT_SEGMENTATION_10\n",
    "PACKETS = 1000 # Number of times per record\n",
    "SUB_WHICH = [\"frames\", \"stacked\"]\n",
    "WEIGHTS = WEIGHTS_IMAGENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenContent = len(os.listdir(DATASET_FOLDER))\n",
    "print(\"Number of items in the folder =\", lenContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VideosDataset Class Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET_FOLDER)\n",
    "ucf = VideosDataset(\n",
    "  DATASET_FOLDER, storageDir=PROJECT_FOLDER,\n",
    "  debug=DEBUG, isGray=IS_GRAY, splits=SPLITS, \n",
    "  batchSize=BATCH_SIZE, inputShape=IMAGE_SHAPE,\n",
    "  datasetKeyword=DATASET_KEYWORD, storeCSV=True,\n",
    "  maxInMemory=MAX_IN_MEMORY, shuffle=SHUFFLE,\n",
    "  segmentation=LIMIT_SEGMENTATION, packets=PACKETS\n",
    ")\n",
    "ucf.ReadVideosMetadata()\n",
    "trainGenerator = ucf.LiveGeneratorImproved('train', SUB_WHICH, \n",
    "  noOfThreads=128, doAugmentation=AUGMENT, \n",
    "  rotate=10, shearRange=5, zoom=5, \n",
    "  horizontalFlip=True, translationRange=5, \n",
    "  changeBrightness=True\n",
    ")\n",
    "validationGenerator = ucf.LiveGeneratorImproved('validation', SUB_WHICH, \n",
    "  noOfThreads=32, doAugmentation=False, \n",
    "  rotate=10, shearRange=5, zoom=5, \n",
    "  horizontalFlip=True, translationRange=5, \n",
    "  changeBrightness=True\n",
    ")\n",
    "noOfClasses = ucf.GetNoOfClasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Class Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models = Models(IMAGE_SHAPE, noOfClasses,\n",
    "  HIDDEN_ACTIVATION, OUTPUT_ACTIVATION, \n",
    "  OPTIMIZER, WEIGHTS_INIT, BATCH_SIZE, NO_OF_EPOCHS,\n",
    "  LOSS, METRICS, STEPS_PER_EPOCH, VALIDATION_STEPS,\n",
    "  loadModel=LOAD_MODEL, datasetKeyword=DATASET_KEYWORD,\n",
    "  resultsFolder=RESULTS_FOLDER, modelName=MODEL_NAME,\n",
    "  savedFolder=SAVED_FOLDER, savedHDF5=SAVED_HDF5,\n",
    "  weights=WEIGHTS, segmentation=LIMIT_SEGMENTATION,\n",
    "  packets=PACKETS\n",
    ")\n",
    "\n",
    "models.CompileModel()\n",
    "models.FitModel(trainGenerator, validationGenerator)\n",
    "\n",
    "del models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
